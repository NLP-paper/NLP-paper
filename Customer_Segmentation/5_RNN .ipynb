{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for detecting the fraud transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator OrdinalEncoder from version 0.21.2 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.21.2 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('data.pickle','rb') as load:\n",
    "    data=pickle.load(load)\n",
    "with open('le.pickle','rb') as load:\n",
    "    le=pickle.load(load)\n",
    "with open('ohe.pickle','rb') as load:\n",
    "    ohe=pickle.load(load)\n",
    "with open('train_test_index.pickle','rb') as load:\n",
    "    train_test_index=pickle.load(load)\n",
    "with open('feature_final.pickle','rb') as load:\n",
    "    feature_final=pickle.load(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will use transaction date time feature as the time step. \n",
    "## Previously, all samples are taken as iid. By involving the time step feature, all data can be seen as time series data!\n",
    "## The transaction sequence probably contain some sequential pattern. When we train RF and Xgboost this pattern may lost.\n",
    "## In order to feed the sequential data into RNN, we have to reshape the samples to 3D: (samples, time_step, input_features)\n",
    "## Now let's reshape sample first! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>acqCountry</th>\n",
       "      <th>cardPresent</th>\n",
       "      <th>merchantCategoryCode</th>\n",
       "      <th>merchantCountryCode</th>\n",
       "      <th>cardCVV</th>\n",
       "      <th>cardLast4Digits</th>\n",
       "      <th>merchantName</th>\n",
       "      <th>posConditionCode</th>\n",
       "      <th>posEntryMode</th>\n",
       "      <th>...</th>\n",
       "      <th>dateOfLastAddressChange_year</th>\n",
       "      <th>dateOfLastAddressChange_month</th>\n",
       "      <th>transactionDateTime_year</th>\n",
       "      <th>transactionDateTime_month</th>\n",
       "      <th>transactionDateTime_time</th>\n",
       "      <th>currentBalance</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>creditLimit</th>\n",
       "      <th>transactionDateTime</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>733493772</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>US</td>\n",
       "      <td>492</td>\n",
       "      <td>9184</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>01</td>\n",
       "      <td>05</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>08</td>\n",
       "      <td>2016</td>\n",
       "      <td>01</td>\n",
       "      <td>19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>111.33</td>\n",
       "      <td>5000</td>\n",
       "      <td>2016-01-08T19:04:50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733493772</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>US</td>\n",
       "      <td>492</td>\n",
       "      <td>9184</td>\n",
       "      <td>Uber</td>\n",
       "      <td>01</td>\n",
       "      <td>09</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>08</td>\n",
       "      <td>2016</td>\n",
       "      <td>01</td>\n",
       "      <td>22</td>\n",
       "      <td>111.33</td>\n",
       "      <td>24.75</td>\n",
       "      <td>5000</td>\n",
       "      <td>2016-01-09T22:32:39</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>733493772</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>rideshare</td>\n",
       "      <td>US</td>\n",
       "      <td>492</td>\n",
       "      <td>9184</td>\n",
       "      <td>Lyft</td>\n",
       "      <td>01</td>\n",
       "      <td>05</td>\n",
       "      <td>...</td>\n",
       "      <td>2014</td>\n",
       "      <td>08</td>\n",
       "      <td>2016</td>\n",
       "      <td>01</td>\n",
       "      <td>13</td>\n",
       "      <td>136.08</td>\n",
       "      <td>187.40</td>\n",
       "      <td>5000</td>\n",
       "      <td>2016-01-11T13:36:55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId acqCountry  cardPresent merchantCategoryCode  \\\n",
       "0   733493772         US        False            rideshare   \n",
       "1   733493772         US        False            rideshare   \n",
       "2   733493772         US        False            rideshare   \n",
       "\n",
       "  merchantCountryCode  cardCVV  cardLast4Digits merchantName posConditionCode  \\\n",
       "0                  US      492             9184         Lyft               01   \n",
       "1                  US      492             9184         Uber               01   \n",
       "2                  US      492             9184         Lyft               01   \n",
       "\n",
       "  posEntryMode  ... dateOfLastAddressChange_year  \\\n",
       "0           05  ...                         2014   \n",
       "1           09  ...                         2014   \n",
       "2           05  ...                         2014   \n",
       "\n",
       "   dateOfLastAddressChange_month  transactionDateTime_year  \\\n",
       "0                             08                      2016   \n",
       "1                             08                      2016   \n",
       "2                             08                      2016   \n",
       "\n",
       "  transactionDateTime_month transactionDateTime_time currentBalance  \\\n",
       "0                        01                       19           0.00   \n",
       "1                        01                       22         111.33   \n",
       "2                        01                       13         136.08   \n",
       "\n",
       "  transactionAmount creditLimit  transactionDateTime isFraud  \n",
       "0            111.33        5000  2016-01-08T19:04:50    True  \n",
       "1             24.75        5000  2016-01-09T22:32:39   False  \n",
       "2            187.40        5000  2016-01-11T13:36:55   False  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_raw=pd.read_json('transactions.txt',lines=True)\n",
    "data=pd.concat([data,data_raw['transactionDateTime']],axis=1)\n",
    "data=pd.concat([data,data_raw['isFraud']],axis=1)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under sampling \n",
    "### In previous notebook I avoid to resampling data, either over sampling or under samping.\n",
    "### Since from my experience it is waste of time and useless, especially for under sampling. which will lose many data information. I usually just adjust class weights.\n",
    "### But for 3D input of RNN, it is difficult to set up class weight, so I will do under sampleing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11302, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['isFraud']==True].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22604, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_under=data[data['isFraud']==False].sample(11302)\n",
    "data_under=data_under.append(data[data['isFraud']==True])\n",
    "data_under.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each customer has different amount of transactions. Focus on all transaction of a customer, if I sort the transaction date time, then all transaction under a customer is time ordered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_under.copy()\n",
    "data=data.sort_values(by='transactionDateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transactionDateTime can be droped now\n",
    "data.drop(['transactionDateTime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divid entire data into pieses groupby customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=data.groupby('customerId')\n",
    "cust_iter=g.__iter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cust is a list of dataframe, a dataframe contains all transaction under a specific customer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust=[i for i in cust_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cust has 3468 customers of dataframe with all the transaction in each dataframe \n"
     ]
    }
   ],
   "source": [
    "print('the cust has %d customers of dataframe with all the transaction in each dataframe '%len(cust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I decide to set up 10 as time step length after taking all things into account. Think that for each customer, we collect 10  neighboured transactions as a single sample for RNN. The total transaction for a customer is not exactly be multiple of 10. In order to align each sample with a same time step, I need to augment transaction (time step) for each customer to the length of nearest mutiple 10. This augment method is called padding method in NLP where the generated word will be set as padding instead of any other word in dictionary. However, in this task, I prefer to set augmented transaction as the last transaction in this sample, just like set up as the last word in a sentence to feed RNN. This will not affect the order information of transaction much!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So list cust_len is the transaction amount for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_len=np.array([len(i[1]) for i in cust])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cust_len_align is the list of transaction length with nearest multiple 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_len_align=np.ceil(np.array(cust_len)/10)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between cust_len_align and cust_len is the amount of augmented transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9. 9. 9. ... 2. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "cust_augment=cust_len_align-cust_len\n",
    "print(cust_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate new samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad44b63021647928b764225eb0557a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(4636, 10, 26)\n"
     ]
    }
   ],
   "source": [
    "LSTM_sample=[]\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "for i in tqdm(range(len(cust))):\n",
    "    df=cust[i][1]\n",
    "    for j in range(int(cust_augment[i])):\n",
    "        df=df.append(df.iloc[-1,:])\n",
    "    df_list=np.array_split(df,cust_len_align[i]/10)\n",
    "    array_list=[np.array(i) for i in df_list]\n",
    "    LSTM_sample.extend(array_list)\n",
    "LSTM_sample=np.array(LSTM_sample)\n",
    "print(LSTM_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at reorgnized samples for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[100547107, 'US', True, ..., 286.07, 2500, True],\n",
       "        [100547107, 'US', True, ..., 286.07, 2500, True],\n",
       "        [100547107, 'US', True, ..., 286.07, 2500, True],\n",
       "        ...,\n",
       "        [100547107, 'US', True, ..., 286.07, 2500, True],\n",
       "        [100547107, 'US', True, ..., 286.07, 2500, True],\n",
       "        [100547107, 'US', True, ..., 286.07, 2500, True]],\n",
       "\n",
       "       [[100634414, 'US', True, ..., 85.51, 10000, False],\n",
       "        [100634414, 'US', True, ..., 85.51, 10000, False],\n",
       "        [100634414, 'US', True, ..., 85.51, 10000, False],\n",
       "        ...,\n",
       "        [100634414, 'US', True, ..., 85.51, 10000, False],\n",
       "        [100634414, 'US', True, ..., 85.51, 10000, False],\n",
       "        [100634414, 'US', True, ..., 85.51, 10000, False]],\n",
       "\n",
       "       [[101548993, 'US', True, ..., 73.68, 7500, False],\n",
       "        [101548993, 'US', True, ..., 73.68, 7500, False],\n",
       "        [101548993, 'US', True, ..., 73.68, 7500, False],\n",
       "        ...,\n",
       "        [101548993, 'US', True, ..., 73.68, 7500, False],\n",
       "        [101548993, 'US', True, ..., 73.68, 7500, False],\n",
       "        [101548993, 'US', True, ..., 73.68, 7500, False]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[999275549, 'US', False, ..., 252.36, 50000, False],\n",
       "        [999275549, 'US', False, ..., 18.43, 50000, False],\n",
       "        [999275549, 'US', False, ..., 89.14, 50000, True],\n",
       "        ...,\n",
       "        [999275549, 'US', False, ..., 90.51, 50000, True],\n",
       "        [999275549, 'US', False, ..., 90.51, 50000, True],\n",
       "        [999275549, 'US', False, ..., 90.51, 50000, True]],\n",
       "\n",
       "       [[999789077, 'US', True, ..., 62.2, 20000, True],\n",
       "        [999789077, 'US', False, ..., 6.16, 20000, False],\n",
       "        [999789077, 'US', False, ..., 26.88, 20000, False],\n",
       "        ...,\n",
       "        [999789077, 'US', True, ..., 53.04, 20000, False],\n",
       "        [999789077, 'US', True, ..., 53.04, 20000, False],\n",
       "        [999789077, 'US', True, ..., 53.04, 20000, False]],\n",
       "\n",
       "       [[999985343, 'US', True, ..., 122.83, 500, False],\n",
       "        [999985343, 'US', False, ..., 0.0, 500, False],\n",
       "        [999985343, 'US', False, ..., 108.57, 500, False],\n",
       "        ...,\n",
       "        [999985343, 'US', False, ..., 151.42, 500, False],\n",
       "        [999985343, 'US', False, ..., 151.42, 500, False],\n",
       "        [999985343, 'US', False, ..., 151.42, 500, False]]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then I need to encoding! Similarly, I will use the pipeline I created in AutoEncoder to preprocess data in a second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=LSTM_sample[:,:,:-1]\n",
    "y=LSTM_sample[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=123,\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline:\n",
    "    def __init__(self, init_estimator):\n",
    "        self.estimator_list=[]\n",
    "        self.estimator_list.extend(init_estimator)\n",
    "    \n",
    "    def add_estimator(self,estimator):# the adding order must be ordinal_encoding, one-hot, SVD, normalize \n",
    "        self.estimator_list.extend(estimator)\n",
    "        \n",
    "    def feature_encoding(self,df,line):\n",
    "        if len(self.estimator_list)>=2:\n",
    "            le_array=self.estimator_list[0].transform(df[:,:line])\n",
    "            le_array=np.append(le_array,np.array(df[:,line:]),axis=1)\n",
    "            ohe_sparse=self.estimator_list[1].transform(le_array).toarray()\n",
    "            return le_array,ohe_sparse\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def feature_processing(self,df,line):\n",
    "        if len(self.estimator_list)>=4:\n",
    "            ohe_sparse=self.feature_encoding(df,line)\n",
    "            svd_array=self.estimator_list[2].transform(ohe_sparse)\n",
    "            std_array=self.estimator_list[3].transform(svd_array)\n",
    "            return std_array\n",
    "        else:\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip=pipeline([le,ohe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f0c1f039ab4458808ea02d513e2c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3708), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_le=[]\n",
    "x_train_ohe=[]\n",
    "for i in tqdm(range(len(x_train))):\n",
    "    le_row,ohe_row=pip.feature_encoding(x_train[i],22)\n",
    "    x_train_le.append(le_row)  \n",
    "    x_train_ohe.append(ohe_row)   \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3708, 10, 25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_le=np.array(x_train_le)\n",
    "x_train_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3708, 10, 13678)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_ohe=np.array(x_train_ohe)\n",
    "x_train_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3708, 10, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y_train.reshape((-1,10,1))\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to feed high dimension data into RNN, I have two options!\n",
    "### 1. use SVD or PCA to reduce dimension of onehotencoder data\n",
    "### 2. use the onehotencoder directly, that will cause too many weights in DNN. \n",
    "### 3. I will drrectly train it by onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 10, 100)           5511600   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 50)            30200     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 10, 30)            9720      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 1)             31        \n",
      "=================================================================\n",
      "Total params: 5,551,551\n",
      "Trainable params: 5,551,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "n_steps = 10\n",
    "n_inputs = 13678\n",
    "n_neurons = 10\n",
    "n_outputs = 1\n",
    "keep_prob=0.9\n",
    "lr=0.001\n",
    "epochs=10\n",
    "batch_size=300\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(100,return_sequences=True,input_shape=(n_steps,n_inputs)))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(30,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1,activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',recall,precision])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2595 samples, validate on 1113 samples\n",
      "Epoch 1/10\n",
      " - 16s - loss: 0.6880 - acc: 0.5627 - recall: 0.0775 - precision: 0.5148 - val_loss: 0.6808 - val_acc: 0.5659 - val_recall: 0.0150 - val_precision: 0.4986\n",
      "\n",
      "Epoch 00001: val_recall improved from inf to 0.01504, saving model to lstm_best.h5\n",
      "Epoch 2/10\n",
      " - 12s - loss: 0.6832 - acc: 0.5642 - recall: 0.0309 - precision: 0.5821 - val_loss: 0.6786 - val_acc: 0.5670 - val_recall: 0.0179 - val_precision: 0.5523\n",
      "\n",
      "Epoch 00002: val_recall did not improve from 0.01504\n",
      "Epoch 3/10\n",
      " - 12s - loss: 0.6824 - acc: 0.5654 - recall: 0.0138 - precision: 0.7002 - val_loss: 0.6772 - val_acc: 0.5650 - val_recall: 0.0047 - val_precision: 0.4103\n",
      "\n",
      "Epoch 00003: val_recall improved from 0.01504 to 0.00472, saving model to lstm_best.h5\n",
      "Epoch 4/10\n",
      " - 13s - loss: 0.6808 - acc: 0.5648 - recall: 0.0127 - precision: 0.6676 - val_loss: 0.6813 - val_acc: 0.5710 - val_recall: 0.0525 - val_precision: 0.5700\n",
      "\n",
      "Epoch 00004: val_recall did not improve from 0.00472\n",
      "Epoch 5/10\n",
      " - 14s - loss: 0.6799 - acc: 0.5680 - recall: 0.0508 - precision: 0.5921 - val_loss: 0.6773 - val_acc: 0.5730 - val_recall: 0.0601 - val_precision: 0.5850\n",
      "\n",
      "Epoch 00005: val_recall did not improve from 0.00472\n",
      "Epoch 6/10\n",
      " - 12s - loss: 0.6805 - acc: 0.5630 - recall: 0.0831 - precision: 0.6300 - val_loss: 0.6791 - val_acc: 0.5624 - val_recall: 0.3198 - val_precision: 0.4936\n",
      "\n",
      "Epoch 00006: val_recall did not improve from 0.00472\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c102c7e80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##earlystoping\n",
    "es=EarlyStopping(monitor='val_loss',mode='auto',verbose=1,patience=3)\n",
    "##save best model\n",
    "mc=ModelCheckpoint('lstm_best.h5',monitor='val_recall',mode='min',verbose=1,save_best_only=True)\n",
    "###training\n",
    "model.fit(x_train_ohe,y_train,epochs=epochs,validation_split=0.3,\n",
    "          batch_size=batch_size,verbose=2,callbacks=[mc,es],\n",
    "          ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's predict in the test dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 10, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ced43e704cc4449bcf31d969e71ec2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=928), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_test_ohe=[]\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    le_row,ohe_row=pip.feature_encoding(x_test[i],22)\n",
    "    x_test_ohe.append(ohe_row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 10, 13678)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_ohe=np.array(x_test_ohe)\n",
    "x_test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict=model.predict(x_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict=(y_test_predict>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 10, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9280,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict=y_test_predict.reshape((-1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.reshape((-1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3876, 1295],\n",
       "       [2718, 1391]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just a demo, for the future work. we can try more models Bidreiction RNN, Attention layer on RNN, etc!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
